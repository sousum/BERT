{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ca6rYUxNi_MO"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "76HfPILdC5lD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1h4YVFfDd1t",
    "outputId": "c4b95386-55b7-4e73-cdc3-0675434f0b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
      "\r",
      "\u001b[K     |████████                        | 10kB 25.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 20kB 33.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 30kB 25.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 40kB 22.5MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 6.6MB/s \n",
      "\u001b[?25hCollecting py-params>=0.9.6\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
      "Collecting params-flow>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=e471d641233a32afd309170bc846ea052f7d524930400fed3114292654a91114\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=732673184c1f4a65ad9e318880c9c1ad92e258ceaeb1e5f0bc2af86393d0ad44\n",
      "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=19730bca66f4593bf3f4cb023d791fb583f406ef470b6a16a04ddd2ec09ec4e6\n",
      "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
      "Successfully built bert-for-tf2 py-params params-flow\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
      "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 18.5MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qMqTwu9jENrO"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0_xu0I3jFP9"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v60JTFKojIq5"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTtO7NkPjKUd"
   },
   "source": [
    "We import files from our personal Google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRCxQui8Gqi_",
    "outputId": "ccfc83e7-0b64-4c11-e7b3-edbf52d9d923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6iT5nxDHLRz"
   },
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(\n",
    "    \"/content/drive/My Drive/projects/BERT/sentiment_data/train.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKnCVewUIBkc"
   },
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"date\", \"query\", \"user\"],\n",
    "          axis=1,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Quzx5tnjUtl"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8hlexmRjXIS"
   },
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBSUDL-UP-W_"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jiMaQsLWiTS"
   },
   "outputs": [],
   "source": [
    "data_clean = [clean_tweet(tweet) for tweet in data.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaqLE0fdWtni"
   },
   "outputs": [],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6eh7sIquja5t"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K59PriX4jgBV"
   },
   "source": [
    "We need to create a BERT layer to have access to meta data for the tokenizer (like vocab size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wry-st-HMN0"
   },
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LggMv7k7Z3Ij"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sent):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGfTo5uIa2is"
   },
   "outputs": [],
   "source": [
    "data_inputs = [encode_sentence(sentence) for sentence in data_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-4oGSu5jxUi"
   },
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLg0Z7QOj_YZ"
   },
   "source": [
    "We will create padded batches (so we pad sentences for each batch independently), this way we add the minimum of padding tokens possible. For that, we sort sentences by length, apply padded_batches and then shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HS_f6gWsLfLM"
   },
   "outputs": [],
   "source": [
    "data_with_len = [[sent, data_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(data_inputs)]\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x: x[2])\n",
    "sorted_all = [(sent_lab[0], sent_lab[1])\n",
    "              for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ry0uJJg8lSQR"
   },
   "outputs": [],
   "source": [
    "all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n",
    "                                             output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "cF74g5hpYzaZ",
    "outputId": "93c6cf91-3cea-4939-c5fd-5824d8bc464b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=int32, numpy=\n",
       " array([ 2053,  2028,  2000,  3422, 19081,  2007,  3892,  1012],\n",
       "       dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(all_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzHAhlfTlrcj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrPqJeYpmfcv"
   },
   "outputs": [],
   "source": [
    "NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxONsFVHkFLU"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6DD3k3qPLDQ"
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x) # batch_size, nb_filters, seq_len-1)\n",
    "        x_1 = self.pool(x_1) # (batch_size, nb_filters)\n",
    "        x_2 = self.trigram(x) # batch_size, nb_filters, seq_len-2)\n",
    "        x_2 = self.pool(x_2) # (batch_size, nb_filters)\n",
    "        x_3 = self.fourgram(x) # batch_size, nb_filters, seq_len-3)\n",
    "        x_3 = self.pool(x_3) # (batch_size, nb_filters)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSix1l4jkIxp"
   },
   "source": [
    "# Stage 4: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhfUFvWEPOIf"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMtdiWmwv6rD"
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6apbd7FrwPYo"
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "78cceSGCw1XC",
    "outputId": "7a65cd56-f31f-4133-f06b-40925ae47750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./drive/My Drive/projects/BERT/ckpt_bert_tok/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YIF5trzx7RA"
   },
   "outputs": [],
   "source": [
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}.\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "WrT8oWZzQNmW",
    "outputId": "bb67b680-42ec-4f0d-afbf-6a8eecaa54fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "  44240/Unknown - 3050s 69ms/step - loss: 0.4145 - accuracy: 0.8115Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_tok/.\n",
      "44240/44240 [==============================] - 3051s 69ms/step - loss: 0.4145 - accuracy: 0.8115\n",
      "Epoch 2/5\n",
      "44239/44240 [============================>.] - ETA: 0s - loss: 0.3715 - accuracy: 0.8362Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_tok/.\n",
      "44240/44240 [==============================] - 2991s 68ms/step - loss: 0.3715 - accuracy: 0.8362\n",
      "Epoch 3/5\n",
      "44239/44240 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.8539Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_tok/.\n",
      "44240/44240 [==============================] - 3000s 68ms/step - loss: 0.3382 - accuracy: 0.8539\n",
      "Epoch 4/5\n",
      "44239/44240 [============================>.] - ETA: 0s - loss: 0.3048 - accuracy: 0.8701Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_tok/.\n",
      "44240/44240 [==============================] - 3084s 70ms/step - loss: 0.3048 - accuracy: 0.8701\n",
      "Epoch 5/5\n",
      "44239/44240 [============================>.] - ETA: 0s - loss: 0.2736 - accuracy: 0.8846Checkpoint saved at ./drive/My Drive/projects/BERT/ckpt_bert_tok/.\n",
      "44240/44240 [==============================] - 3078s 70ms/step - loss: 0.2736 - accuracy: 0.8846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb029a9a9e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(train_dataset,\n",
    "         epochs=NB_EPOCHS,\n",
    "         callbacks=[MyCustomCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IiDW919kQQK"
   },
   "source": [
    "# Stage 5: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MthhNfnG1TPV"
   },
   "outputs": [],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-jrRvtl1xuk"
   },
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "    inputs = tf.expand_dims(tokens, 0)\n",
    "\n",
    "    output = Dcnn(inputs, training=False)\n",
    "\n",
    "    sentiment = math.floor(output*2)\n",
    "\n",
    "    if sentiment == 0:\n",
    "        print(\"Ouput of the model: {}\\nPredicted sentiment: negative.\".format(\n",
    "            output))\n",
    "    elif sentiment == 1:\n",
    "        print(\"Ouput of the model: {}\\nPredicted sentiment: positive.\".format(\n",
    "            output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "wMyVjgSt3Nze",
    "outputId": "d5d3f432-5f6f-4103-fb58-a986278dac03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ouput of the model: [[0.9957925]]\n",
      "Predicted sentiment: positive.\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"This movie was pretty interesting.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B-4oGSu5jxUi",
    "VxONsFVHkFLU",
    "vSix1l4jkIxp"
   ],
   "name": "Bert_tokenizer",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
